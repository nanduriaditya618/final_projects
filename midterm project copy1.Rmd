---
title: "midterm final"
author: "Aditya Nanduri"
date: "11/5/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## Data loading

```{r cars}

ais <- read.csv("ais.csv")



```


## Data exploration
Analyse independent variables distribution
Convert variable "sex" to factor variable

```{r cars}
ais$Sex <- as.factor(ais$Sex)
ais$Sport <- as.factor(ais$Sport)
hist(ais$Ht)
hist(ais$Wt)
hist(ais$LBM)
hist(ais$RCC)
hist(ais$WCC)
hist(ais$Hc)
hist(ais$Hg)
hist(ais$Ferr)
hist(ais$BMI)
hist(ais$SSF)



cor(ais[,-c(1,3)])
```

## Data Preparation
 Since Ht and Wt data are slightly skewed, applying log to make it normal

```{r }
ais$Ht<-log(ais$Ht)
ais$Wt<-log(ais$Wt)
ais<-ais[,-c(1)]
```

##  MODELING SELECTION PROCESS:

##  Multiple Linear and Penalised regression

```{r }

aispenaliseddata <-ais
n= dim(aispenaliseddata)[1]

names(aispenaliseddata) 


# specify models to consider
#model list specification
LinModel1 = (Bfat ~ Ht)
LinModel2 = (Bfat ~ Ht+Wt)
LinModel3 = (Bfat ~ Ht+Wt+LBM)
LinModel4 = (Bfat ~ Ht+Wt+LBM+RCC)
LinModel5 = (Bfat ~ Ht+Wt+LBM+RCC+WCC)
LinModel6 = (Bfat ~ Ht+Wt+LBM+RCC+WCC+Hc)
LinModel7 = (Bfat ~ Ht+Wt+LBM+RCC+WCC+Hc+Hg)
LinModel8 = (Bfat ~ Ht+Wt+LBM+RCC+WCC+Hc+Hg+Ferr)
LinModel9 = (Bfat ~ Ht+Wt+LBM+RCC+WCC+Hc+Hg+Ferr+BMI)
LinModel10 = (Bfat ~ Ht+Wt+LBM+RCC+WCC+Hc+Hg+Ferr+BMI+SSF)


allLinModels = list(LinModel1,LinModel2,LinModel3,LinModel4,LinModel5,LinModel6,LinModel7,LinModel8,LinModel9,LinModel10)	
nLinmodels = length(allLinModels)

library(glmnet)  # use RR and LASSO modeling commands from package glmnet 
# RR model specification and number
lambdalistRR = c(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5)  # specifies RR models to consider
nRRmodels = length(lambdalistRR)
# LASSO model specification and number
lambdalistLASSO = c(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5)  # specifies LASSO models to consider
nLASSOmodels = length(lambdalistLASSO)

# Elasticnet model specification and number
lambdalistENET = c(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5)  # specifies LASSO models to consider
nENETmodels = length(lambdalistENET)

nmodels = nLinmodels+nRRmodels+nLASSOmodels+nENETmodels

#specify the data set used to perform the model selection
fulldata.in = aispenaliseddata
# set seed for randomizing CV fold selection
set.seed(8, sample.kind = "Rounding")
```

```{r }

###########################
## Full modeling process ##
###########################

# we begin setting up the model-fitting process to use notation that will be
# useful later, "in"side a validation
n.in = dim(fulldata.in)[1]
x.in = model.matrix(Bfat~.,data=fulldata.in)
y.in = fulldata.in[,1]
# number folds and groups for (inner) cross-validation for model-selection
k.in = 10 
   #produce list of group labels
groups.in = c(rep(1:k.in,floor(n.in/k.in))); if(floor(n.in/k.in) != (n.in/k.in)) groups.in = c(groups.in, 1:(n.in%%k.in))
cvgroups.in = sample(groups.in,n.in)  #orders randomly, with seed (8) 
# table(cvgroups.in)  # check correct distribution
allmodelCV.in1 = rep(NA,nmodels) #place-holder for results

```

```{r }


##### cross-validation for model selection ##### reference - Lesson 2

# since linear regression does not have any automatic CV output,
# set up storage for predicted values from the CV splits, across all linear models
allpredictedCV.in = matrix(rep(NA,n.in*nLinmodels),ncol=nLinmodels)

#cycle through all folds:  fit the model to training data, predict test data,
# and store the (cross-validated) predicted values
for (i in 1:k.in)  {
  train.in = (cvgroups.in != i)
  test.in = (cvgroups.in == i)
  #fit each of the linear regression models on training, and predict the test
  for (m in 1:nLinmodels) {
    lmfitCV.in = lm(formula = allLinModels[[m]],data=aispenaliseddata,subset=train.in)
    allpredictedCV.in[test.in,m] = predict.lm(lmfitCV.in,fulldata.in[test.in,])
  }
}
# compute and store the CV(10) values
for (m in 1:nLinmodels) { 
  allmodelCV.in1[m] = mean((allpredictedCV.in[,m]-fulldata.in$Bfat)^2)
}

```

```{r }
##### cross-validation for model selection ##### reference - Lesson 5

#RR cross-validation - uses internal cross-validation function
cvRRglm.in = cv.glmnet(x.in, y.in, lambda=lambdalistRR, alpha = 0, nfolds=k.in, foldid=cvgroups.in)


#LASSO cross-validation - uses internal cross-validation function
cvLASSOglm.in = cv.glmnet(x.in, y.in, lambda=lambdalistLASSO, alpha = 1, nfolds=k.in, foldid=cvgroups.in)

#ENET  cross-validation - uses internal cross-validation function
cvENETglm.in = cv.glmnet(x.in, y.in, lambda=lambdalistENET, alpha = 0.5, nfolds=k.in, foldid=cvgroups.in)


# store CV(10) values, in same numeric order as lambda, in storage spots for CV values
allmodelCV.in1[(1:nRRmodels)+nLinmodels] = cvRRglm.in$cvm[order(cvRRglm.in$lambda)]
# store CV(10) values, in same numeric order as lambda, in storage spots for CV values
allmodelCV.in1[(1:nLASSOmodels)+nRRmodels+nLinmodels] = cvLASSOglm.in$cvm[order(cvLASSOglm.in$lambda)]

# store CV(10) values, in same numeric order as lambda, in storage spots for CV values
allmodelCV.in1[(1:nENETmodels)+nLASSOmodels+nRRmodels+nLinmodels] = cvENETglm.in$cvm[order(cvENETglm.in$lambda)]

```

```{r }
# visualize CV(10) values across all methods
plot(allmodelCV.in1,pch=15); abline(v=c(nLinmodels,nLASSOmodels+.5+nRRmodels+.5+nENETmodels+.5))

```

```{r }
bestmodel.in = (1:nmodels)[order(allmodelCV.in1)[1]]  # actual selection
# state which is best model and minimum CV(10) value
bestmodel.in; min(allmodelCV.in1)

```

```{r }
### finally, fit the best model to the full (available) data ###
if (bestmodel.in <= nLinmodels) {  # then best is one of linear models
  bestfit = lm(formula = allLinModels[[bestmodel.in]],data=fulldata.in)  # fit on all available data
  bestcoef = coef(bestfit)
} else if (bestmodel.in <= nRRmodels+nLinmodels) {  # then best is one of RR models
  bestlambdaRR = (lambdalistRR)[bestmodel.in-nLinmodels]
  bestfit = glmnet(x.in, y.in, alpha = 0,lambda=lambdalistRR)  # fit the model across possible lambda
  bestcoef = coef(bestfit, s = bestlambdaRR) # coefficients for the best model fit
} else if (bestmodel.in <= nRRmodels+nLinmodels+nLASSOmodels) {  # then best is one of LASSO models
  bestlambdaLASSO = (lambdalistLASSO)[bestmodel.in-nLinmodels-nRRmodels]
  bestfit = glmnet(x.in, y.in, alpha = 1,lambda=lambdalistLASSO)  # fit the model across possible lambda
  bestcoef = coef(bestfit, s = bestlambdaLASSO) # coefficients for the best model fit
}else {  # then best is one of Enet models
  bestlambdaEnet = (lambdalistENET)[bestmodel.in-nLinmodels-nRRmodels-nLASSOmodels]
  bestfit = glmnet(x.in, y.in, alpha = .5,lambda=lambdalistENET)  # fit the model across possible lambda
  bestcoef = coef(bestfit, s = lambdalistENET) # coefficients for the best model fit
}

#############################
## End of modeling process ##
#############################
```

```{r }
# summary of best model selected
selectmodelsummary = list(selectmodel = bestmodel.in, selectfit = bestfit, 
                        selectcoef = bestcoef)
selectmodelsummary  # in order to recall the final selected fit after any validation


```

Among Linear and Penalised regression models (RR , Lasso and Elastic net) Lasso regression is the best model with cross validation error of 0.9993979

##  Random Forest 

we have choosen is Random forest as second method since it works best when variables have collinear relations and one of the variable has more inportance than others

```{r cars}
library(randomForest)
set.seed(8, sample.kind = "Rounding")
n = 202
k = 10
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
cvgroups = sample(groups,n)
rf.predictmlrfitcv = rep(0,n)
for (i in 1:k){
  groupi = (cvgroups == i)
  
  ais.rf = randomForest(Bfat~., data=ais[!groupi,], mtry = 4, importance = T)
  rf.predictmlrfitcv[groupi] = predict(ais.rf,newdata = ais[groupi,])
 
}

rfcv1 = sum((rf.predictmlrfitcv-ais$Bfat)^2)/n

print(rfcv1)

```

Random forest  cross validation error is 2.106099 vs best among  regrssion cv error(Lasso) is 0.997455

## 3.0 - MODEL ASSESSMENT FOR HONEST PREDICTION

As a final step in this analysis, we will preform double 10-fold cross-validation to assess an honest expectation of error rate for this model. We first split the enter dataset into 10 folds. One is held as an "outer" test, and the other 9 folds are sent into an "inner" modeling selection process where each of these models are also being fit and tested using a CV10 process. The best model from the "inner" fitting process is selected and used to predict the fold that was held back from the "outer" split. The predicted classes are stored, and once the "outer" CV10 process is complete, a confusion matrix is created and the overall misclassification rate is calculated.


```{r warning=FALSE}

aispenaliseddata <-ais
n= dim(aispenaliseddata)[1]

names(aispenaliseddata) 
# specify models to consider
#model list specification
library(glmnet)  # use RR and LASSO modeling commands from package glmnet 

# LASSO model specification and number
lambdalistLASSO = c(0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5)  # specifies LASSO models to consider
nLASSOmodels = length(lambdalistLASSO)

nrfmodels = 1

nmodels = nLASSOmodels + nrfmodels

###################################################################
##### Double cross-validation for modeling-process assessment #####				 
###################################################################

##### model assessment OUTER shell #####
fulldata.out = aispenaliseddata
k.out = 10 
n.out = dim(fulldata.out)[1]
#define the cross-validation splits 
groups.out = c(rep(1:k.out,floor(n.out/k.out))); if(floor(n.out/k.out) != (n.out/k.out)) groups.out = c(groups.out, 1:(n.out%%k.out))
set.seed(8, sample.kind = "Rounding")
cvgroups.out = sample(groups.out,n.out)  #orders randomly, with seed (8) 

# set up storage for predicted values from the double-cross-validation
allpredictedCV.out = rep(NA,n.out)
# set up storage to see what models are "best" on the inner loops
allbestmodels = rep(NA,k.out)

# loop through outer splits
for (j in 1:k.out)  {  #be careful not to re-use loop indices
  groupj.out = (cvgroups.out == j)
  traindata.out = aispenaliseddata[!groupj.out,]
  trainx.out = model.matrix(Bfat~.,data=traindata.out)
  trainy.out = traindata.out[,1]
  validdata.out = aispenaliseddata[groupj.out,]
  validx.out = model.matrix(Bfat~.,data=validdata.out)
  validy.out = validdata.out[,1]

#specify the data set used to perform the model selection
fulldata.in = traindata.out
# set seed for randomizing CV fold selection
    

x.out  = model.matrix(Bfat~.,data=validdata.out)
y.out = validdata.out[,1]

###########################
## Full modeling process ##
###########################

# we begin setting up the model-fitting process to use notation that will be
# useful later, "in"side a validation
n.in = dim(fulldata.in)[1]
x.in = model.matrix(Bfat~.,data=fulldata.in)
y.in = fulldata.in[,1]
# number folds and groups for (inner) cross-validation for model-selection
k.in = 10 
   #produce list of group labels
groups.in = c(rep(1:k.in,floor(n.in/k.in))); if(floor(n.in/k.in) != (n.in/k.in)) groups.in = c(groups.in, 1:(n.in%%k.in))
cvgroups.in = sample(groups.in,n.in)  #orders randomly, with seed (7) 
# table(cvgroups.in)  # check correct distribution
allmodelCV.in = rep(NA,nmodels) #place-holder for results
#LASSO cross-validation - uses internal cross-validation function
cvLASSOglm.in = cv.glmnet(x.in, y.in, lambda=lambdalistLASSO, alpha = 1, nfolds=k.in, foldid=cvgroups.in)

rf.predictmlrfitcv = rep(0,n.in)
for (i in 1:k.in){
  groupi = (cvgroups.in == i)
  ais.rf = randomForest(Bfat~., data=aispenaliseddata[!groupi,], mtry = 4, importance = T)
  rf.predictmlrfitcv[groupi] = predict(ais.rf,newdata = aispenaliseddata[groupi,])
}

rfcv1 = sum((rf.predictmlrfitcv-aispenaliseddata$Bfat)^2)/n.in

# store CV(10) values, in same numeric order as lambda, in storage spots for CV values
allmodelCV.in[(1:nLASSOmodels)] = cvLASSOglm.in$cvm[order(cvLASSOglm.in$lambda)]
allmodelCV.in[(1:nrfmodels)+nLASSOmodels] = rfcv1
# store CV(10) values, in same numeric order as lambda, in storage spots for CV values
bestmodel.in = (1:nmodels)[order(allmodelCV.in)[1]]  # actual selection
# state which is best model and minimum CV(10) value
bestmodel.in; min(allmodelCV.in)
### finally, fit the best model to the full (available) data ###
if (bestmodel.in <= nLASSOmodels) {  # then best is one of linear models
  bestlambdaLASSO = (lambdalistLASSO)[bestmodel.in]
  bestfit = glmnet(x.in, y.in, alpha = 1,lambda=lambdalistLASSO)  # fit the model across possible lambda
  bestcoef = coef(bestfit, s = bestlambdaLASSO) # coefficients for the best model fit
} else{  # then best is one of RR models
  bestfit = randomForest(Bfat~., data=aispenaliseddata, mtry = 4, importance = T)  # fit on all available data
  bestcoef = coef(bestfit)
} 

#############################
## End of modeling process ##
#############################
  ###   :	:	:	:	:	:	:  ###
  ### resulting in bestmodel.in ###
  
  allbestmodels[j] = bestmodel.in
  
  if (bestmodel.in <= nLASSOmodels) {  # then best is one of linear models
    LASSOfit = glmnet(x.out, y.out, alpha = 1,lambda=lambdalistLASSO)
    allpredictedCV.out[groupj.out] = predict(LASSOfit,newx=x.out,s=.5)
     
    
  } else  {  # then best is one of Random forest
    allpredictedCV.out[groupj.out] =  predict(ais.rf,newdata = validdata.out)
     
  } 
}

# for curiosity, we can see the models that were "best" on each of the inner splits
allbestmodels

#assessment
y.out = fulldata.out$Bfat
CV.out = sum((allpredictedCV.out-y.out)^2)/n.out; CV.out
R2.out = 1-sum((allpredictedCV.out-y.out)^2)/sum((y.out-mean(y.out))^2); R2.out
```

```{r cars}
bestfit

```

```{r cars}

cvLASSOglm.in1$cvm[order(cvLASSOglm.in1$lambda)]

```

```{r cars}
allmodelCV.in
cvRRglm.in$cvm[order(cvRRglm.in$lambda)]
cvENETglm.in$cvm[order(cvENETglm.in$lambda)]
(1:nmodels)[order(allmodelCV.in)[1]] 
min(allmodelCV.in)

```

```{r cars}
allmodelCV.in1
rfcv1
```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```

```{r cars}

```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
